
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 3: Priors &#8212; MAS2903 - Notes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=bd9e20870c6007c4c509" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=bd9e20870c6007c4c509"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/chapter3';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Chapter 4: Bayesian inference" href="chapter4.html" />
    <link rel="prev" title="Chapter 2: Bayes Theorem for Distributions" href="chapter2.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <header>
  
    <div class="bd-header navbar navbar-expand-lg bd-navbar">
    </div>
  
  </header>

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="MAS2903 - Notes - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="MAS2903 - Notes - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Preface: What is Bayesian Statistics?
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="chapter1.html">Chapter 1: Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter2.html">Chapter 2: Bayes Theorem for Distributions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Chapter 3: Priors</a></li>
<li class="toctree-l1"><a class="reference internal" href="chapter4.html">Chapter 4: Bayesian inference</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcontent/chapter3.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button"
   title="Open an issue"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/content/chapter3.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 3: Priors</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-1-introduction">Section 3.1: Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-2-informative-priors">Section 3.2: Informative Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-3-1-informative-prior">Definition 3.1: Informative Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-1">Example 3.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-2">Example 3.2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-3-parameter-constraints">Section 3.3: Parameter constraints</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-3-2-truncated-distribution">Definition 3.2: Truncated Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-3">Example 3.3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference-with-truncated-priors">Bayesian Inference with Truncated Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#theorem-3-1-truncated-posterior">Theorem 3.1: Truncated Posterior</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof">Proof</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-4">Example 3.4</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-5">Example 3.5</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-4-uninformative-priors">Section 3.4: Uninformative Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vague-prior-knowledge">Vague Prior Knowledge</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-6">Example 3.6</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-7">Example 3.7</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-ignorance">Prior Ignorance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-8">Example 3.8</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-9">Example 3.9</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-10">Example 3.10</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-11">Example 3.11</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-5-asymptotic-posterior-distribution">Section 3.5: Asymptotic posterior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theorem-3-2-asymptotic-posterior">Theorem 3.2: Asymptotic posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Proof</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-12">Example 3.12</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-13">Example 3.13</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-14">Example 3.14</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="chapter-3-priors">
<h1>Chapter 3: Priors<a class="headerlink" href="#chapter-3-priors" title="Link to this heading">#</a></h1>
<section id="section-3-1-introduction">
<h2>Section 3.1: Introduction<a class="headerlink" href="#section-3-1-introduction" title="Link to this heading">#</a></h2>
<p>In this chapter we will consider different approaches about how to
construct or choose a suitable prior distribution <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> for our
parameter of interest <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>For example, why did we use a <span class="math notranslate nohighlight">\(\mathrm{Beta}(77,5)\)</span> distribution for
<span class="math notranslate nohighlight">\(\theta\)</span> in the music expert example (page 31)? Why did we use a
<span class="math notranslate nohighlight">\(\mathrm{Beta}(2.5,12)\)</span> distribution for <span class="math notranslate nohighlight">\(\theta\)</span> in the example about
the video game pirate (page 34)? And why did we assume a
<span class="math notranslate nohighlight">\(\mathrm{Gamma}(10,4000)\)</span> distribution for <span class="math notranslate nohighlight">\(\theta\)</span> in the earthquake
example (page 37)?</p>
<p>We will consider the case of <em>informative priors</em>, where expert opinion,
for example, gives us good reason to believe that some values in a
permissible range for <span class="math notranslate nohighlight">\(\theta\)</span> are more likely to occur than others. In
particular, we will revisit the examples about the music expert, the
video game pirate and earthquakes in Chapter 2.</p>
<p>Of course, sometimes it might be very difficult to properly elicit a
prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>. For example, there may be no expert
available to help guide your choice of distribution. In this case, the
chosen prior distribution <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> might be one which keeps the
mathematics simple when operating Bayes Theorem, whilst also assuming a
large or “infinite” variance for <span class="math notranslate nohighlight">\(\theta\)</span> (<em>vague prior knowledge</em>).
Alternatively, a prior which assumes all values of <span class="math notranslate nohighlight">\(\theta\)</span> are equally
likely could be used to represent complete <em>prior ignorance</em>, as in the
example about the possibly biased coin (Example 2.1, page 28).</p>
<p>In this chapter we will also consider the construction of priors for
<span class="math notranslate nohighlight">\(\theta\)</span> under certain parameter constraints, including the construction
of <em>truncated priors</em>.</p>
</section>
<section id="section-3-2-informative-priors">
<h2>Section 3.2: Informative Priors<a class="headerlink" href="#section-3-2-informative-priors" title="Link to this heading">#</a></h2>
<section id="definition-3-1-informative-prior">
<h3>Definition 3.1: Informative Prior<a class="headerlink" href="#definition-3-1-informative-prior" title="Link to this heading">#</a></h3>
<p><br />
We have <em>substantial prior information</em> for <span class="math notranslate nohighlight">\(\theta\)</span> when the prior
distribution <em>dominates</em> the posterior distribution, that is
<span class="math notranslate nohighlight">\(\pi(\theta|\underline{x})\sim\pi(\theta)\)</span>.</p>
<p><img alt="Prior (dashed) and posterior (solid) densities for the music expert'sskill for the ." src="../_images/priorposterior1.svg" /></p>
<p>An example of an informative prior was given in Example  where a music
expert was trying to distinguish between pages from Mozart and Haydn
scores. Figure  shows the prior and posterior distributions for
<span class="math notranslate nohighlight">\(\theta\)</span>, the probability that the expert makes the correct choice.
Notice the similarity between the prior and posterior distributions.
Observing the data has not altered our beliefs about <span class="math notranslate nohighlight">\(\theta\)</span> very much.</p>
<p>When we have prior information there can be some difficulties:</p>
<ol class="arabic simple">
<li><p>The practical formulation of the prior distribution from expert
opinions — coherently specifying prior beliefs in the form of a
probability distribution is far from straightforward. This is known
as <em>prior elicitation</em>.</p></li>
<li><p>The intractability of the mathematics in deriving the posterior
distribution — though with modern computing facilities this is
less of a problem,</p></li>
</ol>
<p>In the following two examples, we consider the specification of
informative priors from information provided by experts. These are
examples of <em>prior elicitation</em>.</p>
</section>
<section id="example-3-1">
<h3>Example 3.1<a class="headerlink" href="#example-3-1" title="Link to this heading">#</a></h3>
<p><br />
Let us return to Example of Chapter 2. Recall that we were given some
data on the “waiting times”, in days, between 21 earthquakes, and we
discussed why an exponential distribution <span class="math notranslate nohighlight">\(\mathrm{Exp}(\theta)\)</span> might
be appropriate to model the waiting times. Further, we were told that an
expert on earthquakes has prior beliefs about the rate <span class="math notranslate nohighlight">\(\theta\)</span>,
described by a <span class="math notranslate nohighlight">\(\mathrm{Gamma}(10,4000)\)</span> distribution; a plot of this
prior is shown in Figure . Where did this prior distribution come from?</p>
<p>Suppose the expert tells us that earthquakes in the region we are
interested in usually occur less than once per year; in fact, they occur
on average once every 400 days. This gives us a rate of occurrence of
about 1/400 = 0.0025 per day (to match the “daily” units given above).
Further, he is fairly certain about this and specifies a very small
variance of <span class="math notranslate nohighlight">\(6.25 \times 10^{-7}\)</span>.</p>
<p>A <span class="math notranslate nohighlight">\(\mathrm{Gamma}(a,b)\)</span> distribution seems sensible, since we can’t
observe a negative daily earthquake rate and the Gamma distribution is
specified over positive values only. Using the information provided by
the expert, verify our use of <span class="math notranslate nohighlight">\(a=10\)</span> and <span class="math notranslate nohighlight">\(b=4000\)</span>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Recall that if <span class="math notranslate nohighlight">\(\theta \sim \mathrm{Gamma}(a,b)\)</span>, then</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        \text{E}[\theta] &amp;= \frac{a}{b}, \\
        \text{Var}[\theta] &amp;= \frac{a}{b^2}.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">From the prior information supplied by the expert, we
must have <span class="math notranslate nohighlight">\(\text{E}[\theta] = 2.5\times 10^{-3}\)</span> and
<span class="math notranslate nohighlight">\(\text{Var}[\theta] = 6.25\times 10^{-7}\)</span>. This yields a simultaneous
equation in <span class="math notranslate nohighlight">\(a\)</span> and <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        \frac{a}{b} &amp;= 2.5\times 10^{-3} \Rightarrow a = 2.5\times 10^{-3} b, \\
        \frac{a}{b^2} &amp;= 6.25\times 10^{-7} \Rightarrow a = 6.25\times 10^{-7}b^2.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Setting these two equations equal yields a quadratic
equation in <span class="math notranslate nohighlight">\(b\)</span>. Since <span class="math notranslate nohighlight">\(b &gt; 0\)</span>, we are allowed to divide both sides by
<span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
        2.5\times 10^{-3} b &amp;= 6.25\times 10^{-7}b^2 \\
        b &amp;= \frac{2.5\times 10^{-3}}{ 6.25\times 10^{-7}} = 4000.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Therefore, <span class="math notranslate nohighlight">\(a = 2.5\times 10^{-3} \times 4000 = 10\)</span>.</p>
<p class="sd-card-text">So, the prior information provided by the expert is quantified correctly
by</p>
<div class="math notranslate nohighlight">
\[\theta\sim\mathrm{Gamma}(10, 4000),\]</div>
<p class="sd-card-text">as required.</p>
</div>
</details></section>
<section id="example-3-2">
<h3>Example 3.2<a class="headerlink" href="#example-3-2" title="Link to this heading">#</a></h3>
<p><br />
Now let us return to Example of Chapter 2. We considered an experiment
to determine how good a music expert is at distinguishing between pages
from Haydn and Mozart scores; when presented with a score from each
composer, the expert makes the correct choice with probability <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Before conducting the experiment, we were told that the expert is very
competent; in fact, we were told that <span class="math notranslate nohighlight">\(\theta\)</span> should have a prior
distribution peaking at around 0.95 and for which
<span class="math notranslate nohighlight">\(\text{Pr}(\theta&lt;0.8)\)</span> is very small. To achieve this, we assumed that
<span class="math notranslate nohighlight">\(\theta \sim \mathrm{Beta}(77,5)\)</span>, with density given in Figure 2.4. How
did we know a beta distribution would be appropriate? And how did we
figure out the parameters of this distribution, i.e. <span class="math notranslate nohighlight">\(a=77\)</span> and <span class="math notranslate nohighlight">\(b=5\)</span>?</p>
<p>By now, you should understand why we might work with a beta
distribution: in this example, <span class="math notranslate nohighlight">\(\theta\)</span> is a probability and so must lie
in the interval <span class="math notranslate nohighlight">\([0,1]\)</span>, and a beta distribution is defined over this
range. But how did we know that <span class="math notranslate nohighlight">\(a=77\)</span> and <span class="math notranslate nohighlight">\(b=5\)</span> would give the desired
properties for <span class="math notranslate nohighlight">\(\theta\)</span>?</p>
<p>We are told that the mode of the distribution should be around 0.95;
using the formulae on page 25, we can thus write</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\frac{a-1}{a+b-2}    &amp;=&amp; 0.95  \\
                     &amp; &amp;      \nonumber \\
 \Rightarrow a-1     &amp;=&amp;0.95(a+b-2)  \nonumber \\
 \Rightarrow a-0.95a &amp;=&amp;0.95b - 1.9 + 1 \nonumber \\
 \Rightarrow 0.05a   &amp;=&amp;0.95b - 0.9 \nonumber \\
 \Rightarrow   a     &amp;=&amp;19b - 18.
\end{aligned}\end{split}\]</div>
<p>We are also told that <span class="math notranslate nohighlight">\(\text{Pr}(\theta&lt;0.8)\)</span> must be
small. In fact, suppose we are told that <span class="math notranslate nohighlight">\(\theta&lt;0.8\)</span> might occur with
probability 0.0001. This means that if we integrate the probability
density function for our beta distribution between 0 and 0.8, we would
get 0.0001; from Equation (2.1) on page 25, we can write this as</p>
<div class="math notranslate nohighlight">
\[\int_{0}^{0.8} \frac{\theta^{a-1}(1-\theta)^{b-1}}{\mathrm{B}(a,b)} d\theta = 0.0001.\]</div>
<p>Now, setting <span class="math notranslate nohighlight">\(a = 19b-18\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\int_{0}^{0.8} \frac{\theta^{(19b-18)-1}(1-\theta)^{b-1}}{\mathrm{B}(19b-18,b)} d\theta = 0.0001.\]</div>
<p>We have set the cumulative distribution function for a
<span class="math notranslate nohighlight">\(\mathrm{Beta}(19b-18,b)\)</span> random variable, evaluated at 0.8, equal to
0.0001 and solve for <span class="math notranslate nohighlight">\(b\)</span>. Although this would be rather tricky to do by
hand, we can do this quite easily in <code class="docutils literal notranslate"><span class="pre">R</span></code>. Recall that the <code class="docutils literal notranslate"><span class="pre">R</span></code> command
<code class="docutils literal notranslate"><span class="pre">dbeta(x,a,b)</span></code> evaluates the density of the <span class="math notranslate nohighlight">\(\mathrm{Beta}(a,b)\)</span>
distribution at the point <code class="docutils literal notranslate"><span class="pre">x</span></code> (see page 25); the command <code class="docutils literal notranslate"><span class="pre">pbeta(x,a,b)</span></code>
evaluates the corresponding cumulative distribution function at <code class="docutils literal notranslate"><span class="pre">x</span></code>.
First of all, we re–write (3.3) to set it equal to zero:</p>
<div class="math notranslate nohighlight">
\[
\int_{0}^{0.8} \frac{\theta^{(19b-18)-1}(1-\theta)^{b-1}}{\mathrm{B}(19b-18,b)} d\theta -0.0001 = 0.\]</div>
<p>We then write a function in <code class="docutils literal notranslate"><span class="pre">R</span></code> which computes the left-hand-side of
Equation (3.4):</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">function</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">pbeta</span><span class="p">(</span><span class="mf">0.8</span><span class="p">,</span> <span class="mi">19</span> <span class="o">*</span> <span class="n">b</span> <span class="o">-</span> <span class="mi">18</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="mf">0.0001</span>
    <span class="k">return</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The trick now is to use a numerical procedure to find the root of
<code class="docutils literal notranslate"><span class="pre">answer</span></code> in the <code class="docutils literal notranslate"><span class="pre">R</span></code> function above, i.e. find the value <code class="docutils literal notranslate"><span class="pre">b</span></code> which
equates <code class="docutils literal notranslate"><span class="pre">answer</span></code> to zero (as is required in Equation (3.4). We can do
this using the <code class="docutils literal notranslate"><span class="pre">R</span></code> function <code class="docutils literal notranslate"><span class="pre">uniroot(f,</span> <span class="pre">lower=,</span> <span class="pre">upper=)</span></code>, which uses a
numerical search algorithm to find the root of the expression provided
by the function <code class="docutils literal notranslate"><span class="pre">f</span></code>, having been given a <code class="docutils literal notranslate"><span class="pre">lower</span></code> bound and an <code class="docutils literal notranslate"><span class="pre">upper</span></code>
bound to search within. We know from the formulae on page 25 that
<span class="math notranslate nohighlight">\(a,b&gt;1\)</span> when using expression (3.1) for the mode, so we can search for a
root over some specified domain <span class="math notranslate nohighlight">\(&gt;1\)</span>: for example, we might use
<code class="docutils literal notranslate"><span class="pre">lower=1</span></code> and <code class="docutils literal notranslate"><span class="pre">upper=100</span></code>, giving:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">uniroot</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="err">$</span><span class="n">root</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="mf">5.06513</span>  <span class="err">$</span><span class="n">f</span><span class="o">.</span><span class="n">root</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="mf">6.008134e-09</span>

<span class="err">$</span><span class="nb">iter</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="mi">14</span>  <span class="err">$</span><span class="n">estim</span><span class="o">.</span><span class="n">prec</span>
<span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="mf">6.103516e-05</span>
</pre></div>
</div>
<p>Thus, the solution to Equation (3.3) is <span class="math notranslate nohighlight">\(b=5.06513\)</span>. For simplicity,
rounding down to <span class="math notranslate nohighlight">\(b=5\)</span> and then substituting into (3.2) gives</p>
<div class="math notranslate nohighlight">
\[
a = 19 \times 5 -18 = 77,\]</div>
<p>hence the use of
<span class="math notranslate nohighlight">\(\theta \sim \mathrm{Beta}(77,5)\)</span> in Example in Chapter 2.</p>
<p>There are many more advanced approaches of <em>prior elicitation</em> and it is
still an active area of research.</p>
</section>
</section>
<section id="section-3-3-parameter-constraints">
<h2>Section 3.3: Parameter constraints<a class="headerlink" href="#section-3-3-parameter-constraints" title="Link to this heading">#</a></h2>
<p>Many probability models have constraints on their parameters. For
example, if we are interested in the <em>variance</em> <span class="math notranslate nohighlight">\(\sigma^2\)</span> of
independent normally distributed data</p>
<div class="math notranslate nohighlight">
\[
X_i\,|\, \sigma^2 \sim \mathcal{N}(0, \sigma^2), \quad i = 1,2,\ldots,n,\]</div>
<p>then we must necessarily have <span class="math notranslate nohighlight">\(\sigma^2 &gt; 0\)</span>. This is an example of a
<em>parameter constraint</em>. Other examples include the <span class="math notranslate nohighlight">\(a,b &gt; 0\)</span> parameters
that are used in the <span class="math notranslate nohighlight">\(\mathrm{Gamma}(a,b)\)</span> distribution, or the
parameter <span class="math notranslate nohighlight">\(p\)</span> used in the <span class="math notranslate nohighlight">\(\mathrm{Binomial}(n,p)\)</span> distribution.</p>
<p>In order to perform <em>Bayesian inference</em> on parameters which are
constrained, we need to specify prior distributions which place zero
probability on the regions for which the parameter can’t take.</p>
<p>A general approach to define priors which satisfy such constraints is by
using <em>truncated distributions</em>.</p>
<section id="definition-3-2-truncated-distribution">
<h3>Definition 3.2: Truncated Distribution<a class="headerlink" href="#definition-3-2-truncated-distribution" title="Link to this heading">#</a></h3>
<p><br />
Consider a univariate continuous distribution with density
<span class="math notranslate nohighlight">\(\pi(\theta)\)</span>. Then this distribution <em>truncated to the interval</em>
<span class="math notranslate nohighlight">\([a,b]\)</span> has density:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi_T(\theta) =
    \begin{cases}
      \pi(\theta) / k &amp; \text{for } \theta \in [a,b] \\
      0 &amp; \text{otherwise}
    \end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(k = \int_a^b \pi(\theta) d\theta\)</span>.</p>
<p>Notes:</p>
<ol class="arabic simple">
<li><p>For one-side constraints, for example <span class="math notranslate nohighlight">\(\theta \geq 1\)</span>, we can take
<span class="math notranslate nohighlight">\(a=-\infty\)</span> or <span class="math notranslate nohighlight">\(b=\infty\)</span>.</p></li>
<li><p>Strict inequalities such as <span class="math notranslate nohighlight">\(\theta&gt;1\)</span> can be treated in exactly the
same way as non-strict inequalities such as <span class="math notranslate nohighlight">\(\theta \geq 1\)</span> (this is
because <span class="math notranslate nohighlight">\(\theta\)</span> is a continuous random variable).</p></li>
<li><p>For Bayesian analysis we usually don’t need to actually calculate
<span class="math notranslate nohighlight">\(k\)</span>, as we can simply use:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}
\pi_T(\theta) \propto
      \begin{cases}
        \pi(\theta) &amp; \text{for } \theta \in [a,b] \\
        0 &amp; \text{otherwise}
      \end{cases}\end{split}\]</div>
</section>
<section id="example-3-3">
<h3>Example 3.3<a class="headerlink" href="#example-3-3" title="Link to this heading">#</a></h3>
<p><br />
Suppose <span class="math notranslate nohighlight">\(\theta \sim \mathcal{N}(b,d^2)\)</span>. Find the density of the
truncated distribution for <span class="math notranslate nohighlight">\(\theta&gt;0\)</span>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">From the definition of a truncated distribution, we have, for
<span class="math notranslate nohighlight">\(\theta &gt;0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\pi_T(\theta) = \frac{1}{k \sqrt{2\pi d^2}}\exp\left[-\frac{1}{2d^2}(\theta - b)^2\right],\]</div>
<p class="sd-card-text">where <span class="math notranslate nohighlight">\(k = \int_0^\infty \pi(\theta).\)</span> Note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
       k = \int_0^\infty \pi(\theta) &amp;= \text{Pr}(\theta &gt; 0) \\
       &amp;= 1 - \text{Pr}(\theta &lt; 0) \\
       &amp;= 1 - \text{Pr}\left(\frac{\theta - b}{d} &lt; -\frac{b}{d}\right) \\
       &amp;= 1 - \text{Pr}\left(Z &lt; -\frac{b}{d} \right) \\
       &amp;= 1 - \Phi\left(-\frac{b}{d}\right).
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Here, <span class="math notranslate nohighlight">\(Z \sim \mathcal{N}(0,1)\)</span> and
<span class="math notranslate nohighlight">\(\Phi(z) = \text{Pr}(Z &lt; z)\)</span> is the cdf of <span class="math notranslate nohighlight">\(Z\)</span>. Therefore, for
<span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\pi_T(\theta)  = \frac{1}{\left(1 - \Phi\left(-\frac{b}{d}\right)\right) \sqrt{2\pi d^2}}\exp\left[-\frac{1}{2d^2}(\theta - b)^2\right]\]</div>
</div>
</details><p>Figure  shows a plot of the densities of (a) a <span class="math notranslate nohighlight">\(\mathcal{N}(1,1)\)</span>
distribution and (b) a <span class="math notranslate nohighlight">\(\mathcal{N}(1,1)\)</span> distribution truncated to
<span class="math notranslate nohighlight">\(\theta&gt;0\)</span>. Notice that for <span class="math notranslate nohighlight">\(\theta&gt;0\)</span> the truncated normal’s density
has the same shape as that of the original distribution. However the
truncated density takes proportionately larger values, since both curves
must have an area underneath of one. Also, we can see that the mean of
the truncated distribution will be larger than the mean of the original
distribution and the standard deviation will be smaller.</p>
<p>This is an important general point. Truncating a distribution changes
the mean and variance. Calculating the new values can be difficult.</p>
<p><img alt="Plot of a normal and truncated normaldistribution" src="../_images/truncatednormal.svg" /></p>
</section>
<section id="bayesian-inference-with-truncated-priors">
<h3>Bayesian Inference with Truncated Priors<a class="headerlink" href="#bayesian-inference-with-truncated-priors" title="Link to this heading">#</a></h3>
<p>Suppose we can do a Bayesian analysis for an ordinary prior. Then it’s
easy to do the analysis for a truncated version of this prior by the
following theorem.</p>
<section id="theorem-3-1-truncated-posterior">
<h4>Theorem 3.1: Truncated Posterior<a class="headerlink" href="#theorem-3-1-truncated-posterior" title="Link to this heading">#</a></h4>
<p><br />
Suppose that for a prior <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> the resulting posterior is
<span class="math notranslate nohighlight">\(\pi(\theta | \underline{x})\)</span>. Let <span class="math notranslate nohighlight">\(\pi_T(\theta)\)</span> be the result of
truncating the prior to <span class="math notranslate nohighlight">\([a,b]\)</span>. Then the corresponding posterior is
<span class="math notranslate nohighlight">\(\pi(\theta | \underline{x})\)</span> truncated to <span class="math notranslate nohighlight">\([a,b]\)</span>.</p>
</section>
<section id="proof">
<h4>Proof<a class="headerlink" href="#proof" title="Link to this heading">#</a></h4>
<p>Theorem Let <span class="math notranslate nohighlight">\(\pi'(\theta | \underline{x})\)</span> be the posterior for the
truncated prior. Then:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\pi'(\theta | \underline{x}) &amp;\propto \pi_T(\theta) f(\underline{x}|\theta) \\
&amp; \propto
\begin{cases}
  \pi(\theta) f(\underline{x}|\theta) &amp; \text{for } \theta \in [a,b] \\
  0 &amp; \text{otherwise}
\end{cases} \\
&amp; \propto
\begin{cases}
  \pi(\theta | \underline{x}) &amp; \text{for } \theta \in [a,b] \\
  0 &amp; \text{otherwise}.
\end{cases}
\end{aligned}\end{split}\]</div>
<p>This result makes it easy to do Bayesian analysis under parameter
constraints.</p>
</section>
<section id="example-3-4">
<h4>Example 3.4<a class="headerlink" href="#example-3-4" title="Link to this heading">#</a></h4>
<p><br />
Consider again the case of <span class="math notranslate nohighlight">\(X_i|\theta\sim \mathcal{N}(\theta,h^2)\)</span>,
<span class="math notranslate nohighlight">\(i=1,2,\ldots,n\)</span> (independent) and <span class="math notranslate nohighlight">\(\theta\sim \mathcal{N}(b,d^2)\)</span>, with
<span class="math notranslate nohighlight">\(h\)</span> known. Suppose we knew in advance that the experiment could only
result in positive values for <span class="math notranslate nohighlight">\(\theta\)</span>. Find the posterior distribution
for <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">From the previous theorem, we first compute the posterior distribution
using the non-truncated prior and then we can truncate the posterior.
Recall from Example 2.7 that if we use a non-truncated prior
<span class="math notranslate nohighlight">\(\theta \sim\mathcal{N}(b,d^2)\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
\theta | \underline{x} \sim \mathcal{N}(M, V),\]</div>
<p class="sd-card-text">where</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        M &amp;= \left(\frac{n}{h^2} + \frac{1}{d^2}\right)^{-1}\left(\frac{n\bar{x}}{h^2} + \frac{b}{d^2}\right), \\
        V &amp;= \left(\frac{n}{h^2} + \frac{1}{d^2}\right)^{-1}.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">The posterior using the truncated prior thus has
density, for <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
        \pi'(\theta | \underline{x}) = \frac{1}{\left(1 - \Phi\left(-\frac{M}{\sqrt{V}}\right)\right) \sqrt{2\pi V}}\exp\left[-\frac{1}{2V}(\theta - M)^2\right].
    
\end{aligned}\]</div>
</div>
</details><p>Figure  plots the <span class="math notranslate nohighlight">\(\mathcal{N}(1,1)\)</span> posterior densities using the
original and truncated priors. This plot highlights an important
consequence of using truncated distributions for modelling prior
beliefs, namely, that if particular parameter values are ruled out prior
to seeing the data then they are also ruled out after seeing the data.
Normally, this is not a problem but, as the following example shows,
when the truncation in a prior distribution does not include parameter
values for which the likelihood function is large, misleading
conclusions can be made.</p>
<p><img alt="Plot of a posterior distribution determined using a non-truncated anda truncated prior distribution" src="../_images/truncposterior.svg" /></p>
</section>
<section id="example-3-5">
<h4>Example 3.5<a class="headerlink" href="#example-3-5" title="Link to this heading">#</a></h4>
<p><br />
Consider the case of a large random sample (<span class="math notranslate nohighlight">\(n=1500\)</span>) with sample mean
<span class="math notranslate nohighlight">\(\bar x=157/15\simeq 10.47\)</span> from a normal distribution with known
variance (<span class="math notranslate nohighlight">\(h^2=100\)</span>) and a normal <span class="math notranslate nohighlight">\(\mathcal{N}(3,1)\)</span> prior distribution
for the mean parameter <span class="math notranslate nohighlight">\(\theta\)</span>. The prior probability that <span class="math notranslate nohighlight">\(\theta\)</span>
exceeds 9 is almost zero: <span class="math notranslate nohighlight">\(\text{Pr}(\theta&gt;9)\simeq 10^{-9}\)</span>. So what
is the effect of using a prior distribution which rules out the
extremely unlikely values of <span class="math notranslate nohighlight">\(\theta\)</span> greater than 9? Put another way,
is there much difference between the posterior distributions calculated
using the prior with <span class="math notranslate nohighlight">\(\text{Pr}(\theta&gt;9)\simeq 10^{-9}\)</span> or a truncated
version of the prior with <span class="math notranslate nohighlight">\(\text{Pr}(\theta&gt;9)=0\)</span>?</p>
<p>If no truncation is applied to the prior distribution then Bayes Theorem
produces the posterior distribution
<span class="math notranslate nohighlight">\(\theta|\underline{x}\sim \mathcal{N}(10,0.25^2)\)</span>. The discussion
preceding this example tells us that imposing the truncation <span class="math notranslate nohighlight">\(\theta&lt;9\)</span>
on the prior distribution produces a posterior distribution which is a
<span class="math notranslate nohighlight">\(\mathcal{N}(10,0.25^2)\)</span> distribution, truncated to <span class="math notranslate nohighlight">\(\theta&lt;9\)</span>. Figure 
shows the resulting posterior densities. Clearly, truncating the prior
distribution to <span class="math notranslate nohighlight">\(\theta&lt;9\)</span> has resulted in a posterior distribution
truncated to <span class="math notranslate nohighlight">\(\theta&lt;9\)</span>, even though the likelihood function is very
peaked at <span class="math notranslate nohighlight">\(\theta\simeq 10.47\)</span>. So our prior has ruled out the most
likely values according to the data! Using a prior distribution which
gives very small – but non-zero – probability to values of <span class="math notranslate nohighlight">\(\theta&gt;9\)</span>,
avoids this problem.</p>
<p><img alt="Plot of a posterior distribution determined using a non-truncated anda truncated prior distribution" src="../_images/truncposterior2.svg" /></p>
<p>This example motivates the pragmatic rule: never rule out values for
parameters which are very implausible but not impossible. Instead these
parameter values should be given very small probability density. The
data will then be allowed to inform the posterior distribution about
values of <span class="math notranslate nohighlight">\(\theta\)</span> with very low prior probability (density) but with
very high likelihood.</p>
<div class="tcolorbox docutils">
<p><em>“If a decision-maker thinks something cannot be true and interprets
this to mean it has zero probability, he will never be influenced by any
data, which is surely absurd. So leave a little probability for the moon
being made of green cheese; it can be as small as 1 in a million, but
have it there since otherwise an army of astronauts returning with
samples of the said cheese will leave you unmoved”</em> <em>– Dennis Lindley</em></p>
</div>
</section>
</section>
</section>
<section id="section-3-4-uninformative-priors">
<h2>Section 3.4: Uninformative Priors<a class="headerlink" href="#section-3-4-uninformative-priors" title="Link to this heading">#</a></h2>
<p>If we have very little or no prior information about the model
parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we must still choose a prior distribution in order
to operate Bayes Theorem. Obviously, it would be sensible to choose a
prior distribution which is not concentrated about any particular value,
that is, one with a very large variance. In particular, most of the
information about <span class="math notranslate nohighlight">\(\theta\)</span> will be passed through to the posterior
distribution via the data, and so we have
<span class="math notranslate nohighlight">\(\pi(\theta|\underline{x})\sim f(\underline{x}|\theta)\)</span>.</p>
<p>An example of vague prior knowledge was given in Example where a
possibly biased coin was assessed. Figure shows the prior and posterior
distributions for <span class="math notranslate nohighlight">\(\theta=\text{Pr(Head)}\)</span>. Notice that the prior and
posterior distributions look very different. In fact, in this example,
the posterior distribution is simply a scaled version of the likelihood
function – likelihood functions are not usually proper probability
(density) functions and so scaling is required to ensure that it
integrates to one. Most of our beliefs about <span class="math notranslate nohighlight">\(\theta\)</span> have come from
observing the data.</p>
<p><img alt="Prior (dashed) and posterior (solid) densities for" src="content/images/priorplot1.svg" /></p>
<section id="vague-prior-knowledge">
<h3>Vague Prior Knowledge<a class="headerlink" href="#vague-prior-knowledge" title="Link to this heading">#</a></h3>
<p>We represent vague prior knowledge by using a prior distribution which
is conjugate to the model for <span class="math notranslate nohighlight">\(\underline{x}\)</span> and which has “infinite”
variance.</p>
<section id="example-3-6">
<h4>Example 3.6<a class="headerlink" href="#example-3-6" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from a <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,1/\tau)\)</span>
distribution (with <span class="math notranslate nohighlight">\(\tau\)</span> known). Determine the posterior distribution
assuming a vague prior for <span class="math notranslate nohighlight">\(\mu\)</span>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">The conjugate prior distribution is a normal distribution. We have
already seen that if the prior is <span class="math notranslate nohighlight">\(\mu\sim \mathcal{N}(b,1/d)\)</span> then the
posterior distribution is <span class="math notranslate nohighlight">\(\mu|\underline{x}\sim \mathcal{N}(B,1/D)\)</span>
where</p>
<div class="math notranslate nohighlight">
\[
B=\frac{db+n\tau\bar x}{d+n\tau}\qquad\text{and}\qquad
                D=d+n\tau.\]</div>
<p class="sd-card-text">If we now make our prior knowledge vague
about <span class="math notranslate nohighlight">\(\mu\)</span> by letting the prior variance tend to infinity (<span class="math notranslate nohighlight">\(d\to 0\)</span>),
we obtain</p>
<div class="math notranslate nohighlight">
\[
B\to\bar x\qquad\text{and}\qquad D\to n\tau.\]</div>
<p class="sd-card-text">Therefore,
assuming vague prior knowledge for <span class="math notranslate nohighlight">\(\mu\)</span> results in a
<span class="math notranslate nohighlight">\(\mathcal{N}(\bar x,1/(n\tau))\)</span> posterior distribution. Notice that the
posterior mean is the sample mean (the likelihood mode) and that the
posterior variance <span class="math notranslate nohighlight">\(1/D\to 0\)</span> as <span class="math notranslate nohighlight">\(n\to\infty\)</span>.</p>
</div>
</details></section>
<section id="example-3-7">
<h4>Example 3.7<a class="headerlink" href="#example-3-7" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from an exponential distribution, that
is, <span class="math notranslate nohighlight">\(X_i|\theta\sim \mathrm{Exp}(\theta)\)</span>, <span class="math notranslate nohighlight">\(i=1,2,\ldots,n\)</span>
(independent). Determine the posterior distribution assuming a vague
prior for <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">The conjugate prior distribution is a Gamma distribution. Recall that a
<span class="math notranslate nohighlight">\(\mathrm{Gamma}(g,h)\)</span> distribution has mean <span class="math notranslate nohighlight">\(m=g/h\)</span> and variance
<span class="math notranslate nohighlight">\(v=g/h^2\)</span>. Rearranging these formulae we obtain</p>
<div class="math notranslate nohighlight">
\[
g=\frac{m^2}{v}\quad\quad\text{and}\quad\quad h=\frac{m}{v}.\]</div>
<p class="sd-card-text">Clearly
<span class="math notranslate nohighlight">\(g\to 0\)</span> and <span class="math notranslate nohighlight">\(h\to 0\)</span> as <span class="math notranslate nohighlight">\(v\to\infty\)</span> (for fixed <span class="math notranslate nohighlight">\(m\)</span>). We have seen how
taking a <span class="math notranslate nohighlight">\(\mathrm{Gamma}(g,h)\)</span> prior distribution results in a
<span class="math notranslate nohighlight">\(\mathrm{Gamma}(g+n,h+n\bar x)\)</span> posterior distribution. Therefore,
taking a vague prior distribution will give a
<span class="math notranslate nohighlight">\(\mathrm{Gamma}(n,n\bar x)\)</span> posterior distribution.</p>
<p class="sd-card-text">Note that the posterior mean is <span class="math notranslate nohighlight">\(1/\bar x\)</span> (the likelihood mode) and
that the posterior variance <span class="math notranslate nohighlight">\(1/(n\bar x^2)\to 0\)</span> and <span class="math notranslate nohighlight">\(n\to\infty\)</span>.</p>
</div>
</details></section>
</section>
<section id="prior-ignorance">
<h3>Prior Ignorance<a class="headerlink" href="#prior-ignorance" title="Link to this heading">#</a></h3>
<p>We could represent ignorance by the concept “all values of <span class="math notranslate nohighlight">\(\theta\)</span> are
equally likely”. If <span class="math notranslate nohighlight">\(\theta\)</span> were discrete with <span class="math notranslate nohighlight">\(m\)</span> possible values then
we could assign each value the same probability <span class="math notranslate nohighlight">\(1/m\)</span>. However, if
<span class="math notranslate nohighlight">\(\theta\)</span> is continuous, we need some limiting argument (from the
discrete case). Suppose that <span class="math notranslate nohighlight">\(\theta\)</span> can take values between <span class="math notranslate nohighlight">\(a\)</span> and
<span class="math notranslate nohighlight">\(b\)</span>, where <span class="math notranslate nohighlight">\(-\infty&lt;a&lt;b&lt;\infty\)</span>. Letting all (permitted) values of
<span class="math notranslate nohighlight">\(\theta\)</span> be equally likely results in taking a uniform <span class="math notranslate nohighlight">\(U(a,b)\)</span>
distribution as our prior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>. However, if the
parameter space is not finite then we cannot do this: there is no such
thing as a <span class="math notranslate nohighlight">\(U(-\infty,\infty)\)</span> distribution. Convention suggests that we
should use the “improper” uniform prior distribution</p>
<div class="math notranslate nohighlight">
\[
\pi(\theta)=constant.\]</div>
<p>This distribution is improper because
<span class="math notranslate nohighlight">\(\int_{-\infty}^\infty \pi(\theta)\,d\theta\)</span> is not a convergent
integral, let alone equal to one. We have a similar problem if <span class="math notranslate nohighlight">\(\theta\)</span>
takes positive values — we cannot use a <span class="math notranslate nohighlight">\(U(0,\infty)\)</span> prior
distribution. Now if <span class="math notranslate nohighlight">\(\theta\in (0,\infty)\)</span> then
<span class="math notranslate nohighlight">\(\phi=\log\theta\in (-\infty,\infty)\)</span>, and so we could use an “improper”
uniform prior for <span class="math notranslate nohighlight">\(\phi\)</span>: <span class="math notranslate nohighlight">\(\pi(\phi)=constant\)</span>. In turn, this induces a
distribution on <span class="math notranslate nohighlight">\(\theta\)</span>. Recall the result from Distribution Theory:</p>
<p>Suppose that <span class="math notranslate nohighlight">\(X\)</span> is a random variable with probability density function
<span class="math notranslate nohighlight">\(f_X(x)\)</span>. If <span class="math notranslate nohighlight">\(g\)</span> is a bijective (1–1) function then the random variable
<span class="math notranslate nohighlight">\(Y=g(X)\)</span> has probability density function</p>
<div class="math notranslate nohighlight" id="equation-eq-p14">
<span class="eqno">(11)<a class="headerlink" href="#equation-eq-p14" title="Link to this equation">#</a></span>\[f_Y(y)=f_X\left(g^{-1}(y)\right)\left|\frac{d}{dy}\,g^{-1}(y)\right|.\]</div>
<p>Applying this result to <span class="math notranslate nohighlight">\(\theta=e^\phi\)</span> gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\pi_\theta(\theta)
&amp;=\pi_\phi(\log\theta)\left|\frac{d}{d\theta}\,\log\theta\right|,
\quad\quad\theta&gt;0 \\
&amp;=constant\times\left|\frac{1}{\theta}\right|,\quad\quad\theta&gt;0 \\
&amp;\propto\frac{1}{\theta},\quad\quad\theta&gt;0.
\end{aligned}\end{split}\]</div>
<p>This too is an improper distribution.</p>
<p>There is a drawback of using uniform or improper priors to represent
prior ignorance: if we are “ignorant” about <span class="math notranslate nohighlight">\(\theta\)</span> then we are also
“ignorant” about any function of <span class="math notranslate nohighlight">\(\theta\)</span>, for example, about
<span class="math notranslate nohighlight">\(\phi_1=\theta^3\)</span>, <span class="math notranslate nohighlight">\(\phi_2=e^\theta\)</span>, <span class="math notranslate nohighlight">\(\phi_3=1/\theta\)</span>, … . Is it
possible to choose a distribution where we are ignorant about all these
functions of <span class="math notranslate nohighlight">\(\theta\)</span>? If not, on which function of <span class="math notranslate nohighlight">\(\theta\)</span> should we
place the uniform/improper prior distribution? After a little thought,
it should be clear that there is no distribution which can represent
ignorance for all functions of <span class="math notranslate nohighlight">\(\theta\)</span>. The above example shows that
assigning a uniform/ignorance prior to <span class="math notranslate nohighlight">\(\theta\)</span> means that we do not
have a uniform/ignorance prior for <span class="math notranslate nohighlight">\(e^\theta\)</span>.</p>
<p>A solution to problems of this type was suggested by Sir Harold
Jeffreys. His suggestion was specified in terms of Fisher’s Information:</p>
<div class="math notranslate nohighlight" id="equation-eq-fisher">
<span class="eqno">(12)<a class="headerlink" href="#equation-eq-fisher" title="Link to this equation">#</a></span>\[I(\theta)=E_{\underline{X}|\theta}\left[-
\frac{\partial^2}{\partial\theta^{2}}\,
\log f(\underline{X}|\theta)\right].\]</div>
<p>He recommended that we represent
prior ignorance by the prior distribution</p>
<div class="math notranslate nohighlight">
\[
\pi(\theta)\propto \sqrt{I(\theta)}.\]</div>
<p>Such a prior distribution is
known as a Jeffreys prior distribution.</p>
<section id="example-3-8">
<h4>Example 3.8<a class="headerlink" href="#example-3-8" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from a distribution with probability
density function</p>
<div class="math notranslate nohighlight">
\[
f(x|\theta)=\frac{2x\,e^{-x^2/\theta}}{\theta},\quad\quad x&gt;0,~\theta&gt;0.\]</div>
<p>Determine the Jeffreys prior for this model.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">The likelihood function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        f(\underline{x}|\theta) 
        &amp;=\prod_{i=1}^n \frac{2x_i\,e^{-x_i^2/\theta}}{\theta} \\
        &amp;=\frac{2^n}{\theta^n}\left(\prod_{i=1}^n x_i\right)
        \exp\left\{-\frac{1}{\theta}\sum_{i=1}^n x_i^2\right\}.
        
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Therefore, the log-likelihood function is</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
            \log f(\underline{x} | \theta) &amp;= n\log 2 - n\log \theta + \sum_{i=1}^n\log(x_i) - \frac{1}{\theta}\sum_{i=1}^n x_i^2.
        
\end{aligned}\]</div>
<p class="sd-card-text">The derivatives are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
            \frac{\partial}{\partial \theta}\log f(\underline{x} | \theta) &amp;= -\frac{n}{\theta} + \frac{1}{\theta^2}\sum_{i=1}^n x_i^2, \\
            \frac{\partial^2}{\partial \theta^2}\log f(\underline{x} | \theta) &amp;= \frac{n}{\theta^2} - \frac{2}{\theta^3}\sum_{i=1}^n x_i^2.
        
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">The Fisher information is then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
            I(\theta) &amp;= -\text{E}_{\underline{X}|\theta}\left[ \frac{n}{\theta^2} - \frac{2}{\theta^3} \sum_{i=1}^n X_i^2  \right] \\
            &amp;= -\frac{n}{\theta^2} + \frac{2}{\theta^3}\sum_{i=1}^n \text{E}[X_i^2|\theta].
        
\end{aligned}\end{split}\]</div>
</div>
</details><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        E\left[X^2 \,| \, \theta\right]
        &amp;=\int_0^\infty x^2\times
        \left(\frac{2x\,e^{-x^2/\theta}}{\theta}\right)\, \mathrm{d}x \\
        &amp;=\theta\int_0^\infty y\,e^{-y}\,\mathrm{d}y 
        \quad\quad\text{using the substitution } y=x^2/\theta \\
        &amp;=\theta\times 1 \\
        &amp;=\theta.
        
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Therefore</p>
<div class="math notranslate nohighlight">
\[
I(\theta)
        =-\frac{n}{\theta^2}+\left(\frac{2n}{\theta^3}\times\theta\right) 
        =\frac{n}{\theta^2}.\]</div>
<p class="sd-card-text">Hence, the Jeffreys prior for this model
is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \pi(\theta)&amp;\propto I(\theta)^{1/2} \\
        &amp;\propto \frac{\sqrt{n}}{\theta},\quad\quad\theta&gt;0 \\
        &amp;\propto \frac{1}{\theta},\quad\quad\theta&gt;0.
        
\end{aligned}\end{split}\]</div>
</div>
</details><p>Notice that this distribution is improper since
<span class="math notranslate nohighlight">\(\int_0^\infty d\theta/\theta\)</span> is a divergent integral, and so we cannot
find a constant which ensures that the density function integrates to
one.</p>
</section>
<section id="example-3-9">
<h4>Example 3.9<a class="headerlink" href="#example-3-9" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from an exponential distribution, that
is, <span class="math notranslate nohighlight">\(X_i|\theta\sim \mathrm{Exp}(\theta)\)</span>, <span class="math notranslate nohighlight">\(i=1,2,\ldots,n\)</span>
(independent). Determine the Jeffreys prior for this model.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Recall that the likelihood function is
<span class="math notranslate nohighlight">\(f_{\underline{X}}(\underline{x}|\theta)=\theta^n e^{-n\bar x\theta}\)</span>,
and therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \log f_{\underline{X}}(\underline{x}|\theta) &amp;=n\log\theta -n\bar x\theta \\
    \frac{\partial}{\partial\theta} \log
    f_{\underline{X}}(\underline{x}|\theta) &amp;=\frac{n}{\theta}-n\bar x \\
    \frac{\partial^2}{\partial\theta^2} \log
    f_{\underline{X}}(\underline{x}|\theta)&amp;=-\frac{n}{\theta^2} \\ 
    \Rightarrow I(\theta) &amp;=E_{\underline{X}|\theta}
    \left[-\frac{\partial^2}{\partial\theta^2} \log
    f_{\underline{X}}(\underline{x}|\theta)\right]=\frac{n}{\theta^2}.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Hence, the Jeffreys prior for this model is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \pi(\theta)&amp;\propto I(\theta)^{1/2} \\
    &amp;\propto \frac{\sqrt{n}}{\theta},\quad\quad\theta&gt;0 \\
    &amp;\propto \frac{1}{\theta},\quad\quad\theta&gt;0.
    
\end{aligned}\end{split}\]</div>
</div>
</details><p>Notice that this distribution is improper since
<span class="math notranslate nohighlight">\(\int_0^\infty d\theta/\theta\)</span> is a divergent integral, and so we cannot
find a constant which ensures that the density function integrates to
one.</p>
<p>Notice also that this density is, in fact, a limiting form of a
<span class="math notranslate nohighlight">\(\mathrm{Gamma}(g,h)\)</span> density (ignoring the integration constant) since</p>
<div class="math notranslate nohighlight">
\[
\frac{h^g\,\theta^{g-1}e^{-h\theta}}{\Gamma(g)}\propto
\theta^{g-1}e^{-h\theta} 
\to \frac{1}{\theta},\quad\quad\text{as }g\to 0,~h\to 0.\]</div>
<p>Therefore, we
obtain the same posterior distribution whether we adopt the Jeffreys
prior or vague prior knowledge.</p>
</section>
<section id="example-3-10">
<h4>Example 3.10<a class="headerlink" href="#example-3-10" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from a <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,1/\tau)\)</span>
distribution (with <span class="math notranslate nohighlight">\(\tau\)</span> known). Determine the Jeffreys prior for this
model.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Recall that the likelihood function is</p>
<div class="math notranslate nohighlight">
\[
f_{\underline{X}}(\underline{x}|\mu)=
    \left(\frac{\tau}{2\pi}\right)^{n/2}
    \exp\left\{-\frac{\tau}{2}\sum_{i=1}^n (x_i-\mu)^2\right\},\]</div>
<p class="sd-card-text">and
therefore</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \log\,&amp;f_{\underline{X}}(\underline{x}|\mu)=\frac{n}{2}\log(\tau)-\frac{n}{2}\log(2\pi)
    -\frac{\tau}{2}\sum_{i=1}^n (x_i-\mu)^2 \\
    &amp;\Rightarrow\quad \frac{\partial}{\partial\mu} \log
    f_{\underline{X}}(\underline{x}|\mu)=-\frac{\tau}{2}\times\sum_{i=1}^n -2(x_i-\mu)\\
    &amp;\phantom{\Rightarrow\quad \frac{\partial}{\partial\mu} \log f_{\underline{X}}(\underline{x}|\mu)}
    =\tau\sum_{i=1}^n (x_i-\mu) \\
    &amp;\phantom{\Rightarrow\quad \frac{\partial}{\partial\mu} \log f_{\underline{X}}(\underline{x}|\mu)}
    =n\tau(\bar x-\mu)\\
    &amp;\Rightarrow\quad \frac{\partial^2}{\partial\mu^2} \log
    f_{\underline{X}}(\underline{x}|\mu)=-n\tau\\
    &amp;\Rightarrow\quad I(\mu)=E_{\underline{X}|\mu}
    \left[-\frac{\partial^2}{\partial\mu^2} \log
    f_{\underline{X}}(\underline{x}|\mu)\right]=n\tau.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Hence, the Jeffreys prior for this model is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \pi(\mu)&amp;\propto I(\mu)^{1/2} \\
    &amp;\propto\sqrt{n\tau},\qquad-\infty&lt;\mu&lt;\infty \\
    &amp;= constant,\qquad-\infty&lt;\mu&lt;\infty. 
    
\end{aligned}\end{split}\]</div>
</div>
</details><p>Notice that this distribution is improper since
<span class="math notranslate nohighlight">\(\int_{-\infty}^\infty d\mu\)</span> is a divergent integral, and so we cannot
find a constant which ensures that the density function integrates to
one.</p>
<p>Also it is a limiting form of a <span class="math notranslate nohighlight">\(\mathcal{N}(b,1/d)\)</span> density (ignoring
the integration constant) since</p>
<div class="math notranslate nohighlight">
\[
\left(\frac{d}{2\pi}\right)^{1/2}\exp\left\{-\frac{d}{2}(\mu-b)^2\right\}
\propto
\exp\left\{-\frac{d}{2}(\mu-b)^2\right\}
\to 1,\quad\quad\text{as }d\to 0.\]</div>
<p>Therefore, we obtain the same
posterior distribution whether we adopt the Jeffreys prior or vague
prior knowledge.</p>
<p>As discussed at the beginning of this section on prior ignorance,
Jeffrey’s prior is <strong>invariant to reparameterisation</strong>. The following
example demonstrates an instance of this:</p>
</section>
<section id="example-3-11">
<h4>Example 3.11<a class="headerlink" href="#example-3-11" title="Link to this heading">#</a></h4>
<p>Recall that, from Example 3.9, the Jeffreys prior for the model</p>
<div class="math notranslate nohighlight">
\[
X_i|\theta\sim \mathrm{Exp}(\theta), \quad i = 1,\ldots, n\]</div>
<p>is</p>
<div class="math notranslate nohighlight">
\[
\pi(\theta) \propto \frac{1}{\theta}.\]</div>
<p>Suppose we instead
<strong>reparameterised</strong> our model and instead used the parameter
<span class="math notranslate nohighlight">\(\phi = \log \theta\)</span>. First, using Jeffrey’s prior for <span class="math notranslate nohighlight">\(\theta\)</span> compute
the density of the transformed random variable <span class="math notranslate nohighlight">\(\phi = \log\theta\)</span>.</p>
<p>Finally, compute Jeffrey’s prior using <span class="math notranslate nohighlight">\(\phi\)</span> as the parameter, where
each <span class="math notranslate nohighlight">\(X_i|\phi \sim \mathrm{Exp}(e^{\phi})\)</span>.</p>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Recall the change of variable formula: If <span class="math notranslate nohighlight">\(\theta\)</span> is a random variable
and <span class="math notranslate nohighlight">\(\phi = g(\theta)\)</span> for a bijective and differentiable function <span class="math notranslate nohighlight">\(g\)</span>,
then</p>
<div class="math notranslate nohighlight">
\[
\pi_{\phi}(\phi) = \pi_{\theta}(g^{-1}(\phi))\left|\frac{\partial}{\partial \phi} g^{-1}(\phi)\right|.\]</div>
<p class="sd-card-text">We have <span class="math notranslate nohighlight">\(g(\theta) = \log\theta\)</span> and so <span class="math notranslate nohighlight">\(g^{-1}(\phi) = e^{\phi}\)</span> and
<span class="math notranslate nohighlight">\(\frac{\partial}{\partial\phi}  \left(e^{\phi}\right) = e^{\phi}.\)</span>
Therefore, the density of <span class="math notranslate nohighlight">\(\phi = \log \theta\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\pi_{\phi}(\phi) \propto \frac{1}{e^{\phi}} \times e^{\phi}  = 1.\]</div>
<p class="sd-card-text">The likelihood function of the model
<span class="math notranslate nohighlight">\(X_i|\phi \sim \mathrm{Exp}(e^{\phi}), \quad i=1,\ldots,n\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
             f(\underline{x}|\phi) &amp;= \prod_{i=1}^n e^{\phi} e^{-e^{\phi}x_i} = e^{n\phi} \exp\left\{-n\bar{x}e^{\phi}\right\}.
         
\end{aligned}\]</div>
<p class="sd-card-text">We have
<span class="math notranslate nohighlight">\(\log f(\underline{x}|\phi) = n\phi -  n\bar{x}e^{\phi}.\)</span> The second
derivative with respect to <span class="math notranslate nohighlight">\(\phi\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial^2}{\partial\phi^2} \log f(\underline{x}|\phi) = - n\bar{x}e^{\phi}.\]</div>
<p class="sd-card-text">Therefore, Fishers information is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
            I(\phi) &amp;= -\text{E}_{\underline{X}|\phi}\left[\frac{\partial^2}{\partial\phi^2} \log f(\underline{x}|\phi) \right] \\
            &amp;= \text{E}_{\underline{X}|\phi}[n\bar{X}e^{\phi}].
        
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Note that</p>
<div class="math notranslate nohighlight">
\[
\text{E}[X_i] = e^{-\phi}\]</div>
<p class="sd-card-text">and so
<span class="math notranslate nohighlight">\(I(\phi) = n e^{-\phi} e^{\phi} = n\)</span>. Therefore, Jeffreys prior is
<span class="math notranslate nohighlight">\(\pi(\phi) \propto \sqrt{I(\phi)} = 1\)</span>.</p>
</div>
</details></section>
</section>
</section>
<section id="section-3-5-asymptotic-posterior-distribution">
<h2>Section 3.5: Asymptotic posterior distribution<a class="headerlink" href="#section-3-5-asymptotic-posterior-distribution" title="Link to this heading">#</a></h2>
<p>There are many limiting results in Statistics. The one you will probably
remember is the Central Limit Theorem. This concerns the distribution of
<span class="math notranslate nohighlight">\(\bar X_n\)</span>, the mean of <span class="math notranslate nohighlight">\(n\)</span> independent and identically distributed
random variables (each with known mean <span class="math notranslate nohighlight">\(\mu\)</span> and known variance
<span class="math notranslate nohighlight">\(\sigma^2\)</span>), as the sample size <span class="math notranslate nohighlight">\(n\to\infty\)</span>. It is easy to show that
<span class="math notranslate nohighlight">\(\text{E}[\bar X_n]=\mu\)</span> and <span class="math notranslate nohighlight">\(\text{Var}[\bar X_n]=\sigma^2/n\)</span>, and so</p>
<div class="math notranslate nohighlight">
\[
\text{E}\left[\frac{\sqrt{n}(\bar{X}_{n}-\mu)}{\sigma}\right] = 0 \qquad \mathrm{and} \qquad \text{Var}\left[\frac{\sqrt{n}(\bar{X}_{n}-\mu)}{\sigma}\right]=1.\]</div>
<p>These two equations are true for all values of <span class="math notranslate nohighlight">\(n\)</span>. The important part
of the Central Limit Theorem is the description of the distribution of
<span class="math notranslate nohighlight">\(\sqrt{n}(\bar{X}_{n}-\mu)/\sigma\)</span> as <span class="math notranslate nohighlight">\(n \to \infty\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\sqrt{n}(\bar X_n-\mu)}{\sigma}\stackrel{\cal D} \longrightarrow
\mathcal{N}(0,1)\quad\quad\text{as }n\to\infty.\]</div>
<p>The following theorem
gives a similar result for the posterior distribution.</p>
<section id="theorem-3-2-asymptotic-posterior">
<h3>Theorem 3.2: Asymptotic posterior<a class="headerlink" href="#theorem-3-2-asymptotic-posterior" title="Link to this heading">#</a></h3>
<p><br />
Suppose we have a statistical model <span class="math notranslate nohighlight">\(f(\underline{x}|\theta)\)</span> for data
<span class="math notranslate nohighlight">\(\underline{x} = (x_1, \ldots, x_n)^\top\)</span>, together with a prior
distribution <span class="math notranslate nohighlight">\(\pi(\theta)\)</span> for <span class="math notranslate nohighlight">\(\theta\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[
\sqrt{J(\hat{\theta})}~(\theta-\hat{\theta})|\underline{x} \stackrel{\cal D}
  \longrightarrow \mathcal{N}(0,1)
  \quad\quad\text{as }n\to\infty\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\theta}\)</span> is the
likelihood mode and <span class="math notranslate nohighlight">\(J(\theta)\)</span> is the <em>observed information</em>:</p>
<div class="math notranslate nohighlight">
\[
J(\theta)=-\frac{\partial^2}{\partial\theta^2}\, \log f(\underline{x}|\theta).\]</div>
</section>
<section id="id1">
<h3>Proof<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<p>Theorem Using Bayes Theorem, the posterior distribution for <span class="math notranslate nohighlight">\(\theta\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\pi(\theta|\underline{x}) \propto\pi(\theta)\,f(\underline{x}|\theta).\]</div>
<p>Let <span class="math notranslate nohighlight">\(\psi=\sqrt{n}(\theta-\hat\theta)\)</span> and</p>
<div class="math notranslate nohighlight">
\[
\ell_n(\theta) =\frac{1}{n}\,\log f(\underline{x}|\theta)\]</div>
<p>be the
average log-likelihood per observation, in which case,
<span class="math notranslate nohighlight">\(f(\underline{x}|\theta)=e^{n\ell_n(\theta)}\)</span>. Using , the posterior
distribution of <span class="math notranslate nohighlight">\(\psi\)</span> is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\pi_\psi(\psi|\underline{x})
&amp;=\pi_\theta\left(\hat\theta+\left.\frac{\psi}{\sqrt{n}}\right|\underline{x}\right)
\times\frac{1}{\sqrt{n}} \\
&amp;\propto\pi_\theta\left(\hat\theta+\frac{\psi}{\sqrt{n}}\right)
\exp\left\{n\ell_n\left(\hat\theta+\frac{\psi}{\sqrt{n}}\right)\right\}.
\end{aligned}\end{split}\]</div>
<p>Now taking Taylor series expansions about <span class="math notranslate nohighlight">\(\psi=0\)</span> gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\pi_\theta\left(\hat\theta+\frac{\psi}{\sqrt{n}}\right)
&amp;=\pi_\theta(\hat\theta)+
\pi_\theta'(\hat\theta)\frac{\psi}{\sqrt{n}}
+O\left(\frac{\psi^2}{n}\right) \\
&amp;=\pi_\theta(\hat\theta)\left\{1+O\left(\frac{\psi}{\sqrt{n}}\right)\right\}
\\ \\
n\ell_n\left(\hat\theta+\frac{\psi}{\sqrt{n}}\right)
&amp;=n\left\{\ell_n(\hat\theta)+
\ell_n'(\hat\theta)\frac{\psi}{\sqrt{n}}+
\frac{1}{2}\ell_n''(\hat\theta)\frac{\psi^2}{n}
+O\left(\frac{\psi^3}{n^{3/2}}\right)\right\} \\
&amp;=n\ell_n(\hat\theta)+\frac{1}{2}
\ell_n''(\hat\theta)\psi^2+O\left(\frac{\psi^3}{\sqrt{n}}\right) \\
\end{aligned}\end{split}\]</div>
<p>since <span class="math notranslate nohighlight">\(\ell_n'(\hat\theta)=0\)</span> by definition of the
maximum likelihood estimate. Therefore, retaining only terms in <span class="math notranslate nohighlight">\(\psi\)</span>,
we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\pi_\psi(\psi|\underline{x})
&amp;\propto
\pi_\theta(\hat\theta)\left\{1+O\left(\frac{\psi}{\sqrt{n}}\right)\right\}
\exp\left\{n\ell_n(\hat\theta)+\frac{1}{2}
\ell_n''(\hat\theta)\psi^2+O\left(\frac{\psi^3}{\sqrt{n}}\right)\right\} \\
&amp;\propto
\exp\{n\ell_n(\hat\theta)\}
\exp\left\{\frac{1}{2}\ell_n''(\hat\theta)\psi^2\right\}
\exp\left\{O\left(\frac{\psi^3}{\sqrt{n}}\right)\right\}
\left\{1+O\left(\frac{\psi}{\sqrt{n}}\right)\right\} \\
&amp;\propto
\exp\left\{\frac{1}{2}\ell_n''(\hat\theta)\psi^2\right\}
\left\{1+O\left(\frac{\psi^3}{\sqrt{n}}\right)\right\}
\left\{1+O\left(\frac{\psi}{\sqrt{n}}\right)\right\} \\
&amp;\propto
\exp\left\{\frac{1}{2}\ell_n''(\hat\theta)\psi^2\right\}
\left\{1+O\left(\frac{\psi}{\sqrt{n}}\right)\right\} \\
&amp;\propto
\exp\left\{-\frac{\psi^2}{2[-\ell_n''(\hat\theta)]^{-1}}\right\}
\left\{1+O\left(\frac{\psi}{\sqrt{n}}\right)\right\}. 
\end{aligned}\end{split}\]</div>
<p>Hence</p>
<div class="math notranslate nohighlight">
\[
\pi_\psi(\psi|\underline{x})\to 
k\exp\left\{-\frac{\psi^2}{2[-\ell_n''(\hat\theta)]^{-1}}\right\}
\quad\quad\text{as }n\to\infty.\]</div>
<p>Thus, the limiting form of the
posterior density for <span class="math notranslate nohighlight">\(\psi\)</span> is that of a
<span class="math notranslate nohighlight">\(\mathcal{N}\left(0,[-\ell_n''(\hat\theta)]^{-1}\right)\)</span> distribution.
Hence</p>
<div class="math notranslate nohighlight">
\[
\sqrt{n}(\theta-\hat\theta)|\underline{x}\stackrel{\cal D} \longrightarrow
\mathcal{N}\left(0,[-\ell_n''(\hat\theta)]^{-1}\right)
\quad\quad\text{as }n\to\infty,\]</div>
<p>or, equivalently, since
$n\ell_n(\theta)=\log f(\underline{x}|\theta)</p>
<div class="math notranslate nohighlight">
\[$
\sqrt{J(\hat{\theta})}~\left.\bigl(\theta-\hat\theta\bigr)
\right|\underline{x}\stackrel{\cal D} \longrightarrow \mathcal{N}(0,1)
\quad\quad\text{as }n\to\infty,\]</div>
<p>as required.</p>
</section>
<section id="comments">
<h3>Comments<a class="headerlink" href="#comments" title="Link to this heading">#</a></h3>
<section id="example-3-12">
<h4>Example 3.12<a class="headerlink" href="#example-3-12" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from a distribution with probability
density function</p>
<div class="math notranslate nohighlight">
\[
f(x|\theta)=\frac{2x\,e^{-x^2/\theta}}{\theta},\quad\quad x&gt;0,~\theta&gt;0.\]</div>
<p>Determine the asymptotic posterior distribution for <span class="math notranslate nohighlight">\(\theta\)</span>. Note that
from Example  we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial\theta} \log
f(\underline{x}|\theta)&amp;=-\frac{n}{\theta}+\frac{1}{\theta^2}\sum_{i=1}^n
x_i^2, \\
J(\theta)=-\frac{\partial^2}{\partial\theta^2} \log
f(\underline{x}|\theta)&amp;=-\frac{n}{\theta^2} +\frac{2}{\theta^3}\sum_{i=1}^n
x_i^2 =\frac{n}{\theta^3}\left(-\theta+\frac{2}{n}\sum_{i=1}^n
x_i^2\right).
\end{aligned}\end{split}\]</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Write <span class="math notranslate nohighlight">\(\overline{x^2}=\dfrac{1}{n}\displaystyle\sum_{i=1}^n x_i^2\)</span> then
we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial}{\partial\theta} \log f(\underline{x}|\theta)=0
    \quad\quad&amp;\Longrightarrow\quad\quad
    \hat\theta=\overline{x^2}, \\
    &amp;\Longrightarrow\quad\quad
    J(\hat\theta)=\frac{n}{\left(\overline{x^2}\right)^2} \\
    &amp;\Longrightarrow\quad\quad
    J(\hat\theta)^{-1}=\frac{1}{n}\left(\overline{x^2}\right)^2.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Therefore, for large <span class="math notranslate nohighlight">\(n\)</span>, the (approximate) posterior
distribution for <span class="math notranslate nohighlight">\(\theta\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\theta|\underline{x}\sim \mathcal{N}\left(\overline{x^2},\,
    \frac{1}{n}\left(\overline{x^2}\right)^2\right).\]</div>
</div>
</details></section>
<section id="example-3-13">
<h4>Example 3.13<a class="headerlink" href="#example-3-13" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from an exponential distribution, that
is, <span class="math notranslate nohighlight">\(X_i|\theta\sim \mathrm{Exp}(\theta)\)</span>, <span class="math notranslate nohighlight">\(i=1,2,\ldots,n\)</span>
(independent). Determine the asymptotic posterior distribution for
<span class="math notranslate nohighlight">\(\theta\)</span>. Note that from Example we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial\theta} \log
f(\underline{x}|\theta)&amp;=\frac{n}{\theta}-n\bar x , \\
J(\theta)=-\frac{\partial^2}{\partial\theta^2} \log
f(\underline{x}|\theta)&amp;=\frac{n}{\theta^2}.
\end{aligned}\end{split}\]</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">We have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial}{\partial\theta} \log f(\underline{x}|\theta)=0
    \quad\quad&amp;\Longrightarrow\quad\quad
    \hat\theta=\frac{1}{\bar x} \\
    &amp;\Longrightarrow\quad\quad
    J(\hat\theta)=n\bar x^2 \\
    &amp;\Longrightarrow\quad\quad
    J(\hat\theta)^{-1}=\frac{1}{n\bar x^2}.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Therefore, for large <span class="math notranslate nohighlight">\(n\)</span>, the (approximate) posterior
distribution for <span class="math notranslate nohighlight">\(\theta\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\theta|\underline{x}\sim \mathcal{N}\left(\frac{1}{\bar x},\,\frac{1}{n\bar x^2}\right).\]</div>
</div>
</details><p>Recall that, assuming a vague prior distribution, the posterior
distribution is a <span class="math notranslate nohighlight">\(\mathrm{Gamma}(n,n\bar x)\)</span> distribution, with mean
<span class="math notranslate nohighlight">\(1/\bar x\)</span> and variance <span class="math notranslate nohighlight">\(1/(n\bar x^2)\)</span>. The Central Limit Theorem tells
us that, for large <span class="math notranslate nohighlight">\(n\)</span>, the gamma distribution tends to a normal
distribution, matched, of course, for mean and variance. Therefore, we
have shown that, for large <span class="math notranslate nohighlight">\(n\)</span>, the asymptotic posterior distribution is
the same as the posterior distribution under vague prior knowledge. Not
a surprising result!</p>
</section>
<section id="example-3-14">
<h4>Example 3.14<a class="headerlink" href="#example-3-14" title="Link to this heading">#</a></h4>
<p><br />
Suppose we have a random sample from a <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,1/\tau)\)</span>
distribution (with <span class="math notranslate nohighlight">\(\tau\)</span> known). Determine the asymptotic posterior
distribution for <span class="math notranslate nohighlight">\(\mu\)</span>. Note that from Example  we have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial}{\partial\mu} \log
f(\underline{x}|\mu)&amp;=n\tau(\bar x-\mu), \\
J(\mu)=-\frac{\partial^2}{\partial\mu^2} \log
f(\underline{x}|\mu)&amp;=n\tau.
\end{aligned}\end{split}\]</div>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Solution<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">We have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \frac{\partial}{\partial\mu} \log f(\underline{x}|\mu)=0
    \quad\quad&amp;\Longrightarrow\quad\quad
    \hat\mu=\bar x \\
    &amp;\Longrightarrow\quad\quad
    J(\hat\mu)=n\tau \\
    &amp;\Longrightarrow\quad\quad
    J(\hat\mu)^{-1}=\frac{1}{n\tau}.
    
\end{aligned}\end{split}\]</div>
<p class="sd-card-text">Therefore, for large <span class="math notranslate nohighlight">\(n\)</span>, the (approximate) posterior
distribution for <span class="math notranslate nohighlight">\(\mu\)</span> is</p>
<div class="math notranslate nohighlight">
\[
\mu|\underline{x}\sim \mathcal{N}\left(\bar x,\,\frac{1}{n\tau}\right).\]</div>
</div>
</details><p>Again, we have shown that the asymptotic posterior distribution is the
same as the posterior distribution under vague prior knowledge.</p>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="chapter2.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Chapter 2: Bayes Theorem for Distributions</p>
      </div>
    </a>
    <a class="right-next"
       href="chapter4.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Chapter 4: Bayesian inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-1-introduction">Section 3.1: Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-2-informative-priors">Section 3.2: Informative Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-3-1-informative-prior">Definition 3.1: Informative Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-1">Example 3.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-2">Example 3.2</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-3-parameter-constraints">Section 3.3: Parameter constraints</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#definition-3-2-truncated-distribution">Definition 3.2: Truncated Distribution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-3">Example 3.3</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-inference-with-truncated-priors">Bayesian Inference with Truncated Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#theorem-3-1-truncated-posterior">Theorem 3.1: Truncated Posterior</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof">Proof</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-4">Example 3.4</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-5">Example 3.5</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-4-uninformative-priors">Section 3.4: Uninformative Priors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vague-prior-knowledge">Vague Prior Knowledge</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-6">Example 3.6</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-7">Example 3.7</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-ignorance">Prior Ignorance</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-8">Example 3.8</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-9">Example 3.9</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-10">Example 3.10</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-11">Example 3.11</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#section-3-5-asymptotic-posterior-distribution">Section 3.5: Asymptotic posterior distribution</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#theorem-3-2-asymptotic-posterior">Theorem 3.2: Asymptotic posterior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Proof</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comments">Comments</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-12">Example 3.12</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-13">Example 3.13</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-3-14">Example 3.14</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr Matthew Fisher
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=bd9e20870c6007c4c509"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=bd9e20870c6007c4c509"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>